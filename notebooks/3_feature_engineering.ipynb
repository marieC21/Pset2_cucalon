{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Copia del dataset creada exitosamente. Listo para Feature Engineering\n"
     ]
    }
   ],
   "source": [
    "# Cargar el dataset limpio sin modificarlo\n",
    "file_path = '/Users/chants/Desktop/Pset2_cucalon/data/processed/Airbnb_Cleaned.csv'\n",
    "df_cleaned = pd.read_csv(file_path)\n",
    "\n",
    "# Crear una copia para trabajar en Feature Engineering sin afectar el dataset original\n",
    "df = df_cleaned.copy()\n",
    "\n",
    "print(\"âœ… Copia del dataset creada exitosamente. Listo para Feature Engineering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           amenities  total_amenities\n",
      "0  {\"Wireless Internet\",\"Air conditioning\",Kitche...                5\n",
      "1  {\"Wireless Internet\",\"Air conditioning\",Kitche...                7\n",
      "2  {TV,\"Cable TV\",\"Wireless Internet\",\"Air condit...               11\n",
      "3  {TV,\"Cable TV\",Internet,\"Wireless Internet\",Ki...                8\n",
      "4  {TV,Internet,\"Wireless Internet\",\"Air conditio...                6\n",
      "5  {TV,\"Wireless Internet\",Heating,\"Smoke detecto...                6\n",
      "6  {TV,Internet,\"Wireless Internet\",\"Air conditio...               10\n",
      "7  {TV,\"Cable TV\",\"Wireless Internet\",\"Wheelchair...               14\n",
      "8  {TV,\"Cable TV\",\"Wireless Internet\",\"Pets live ...               14\n",
      "9  {\"Wireless Internet\",\"Air conditioning\",Kitche...                7\n",
      "\n",
      "ðŸ“ˆ CorrelaciÃ³n de total_amenities con log_price:\n",
      "                 total_amenities  log_price\n",
      "total_amenities         1.000000   0.141215\n",
      "log_price               0.141215   1.000000\n"
     ]
    }
   ],
   "source": [
    "import re  #  Importamos regex para extraer valores correctamente\n",
    "\n",
    "# Crear copia del dataset para no modificar el original\n",
    "df_featured = df.copy()\n",
    "\n",
    "# FunciÃ³n para contar el nÃºmero de amenidades\n",
    "def count_amenities(amenities_str):\n",
    "    if isinstance(amenities_str, str):  \n",
    "        # ExpresiÃ³n regular para capturar palabras dentro de comillas dobles \"\"\n",
    "        matches = re.findall(r'\"([^\"]+)\"', amenities_str)  \n",
    "        return len(matches)  #  Retorna la cantidad de amenidades detectadas\n",
    "    return 0  # Si no es string, devuelve 0\n",
    "\n",
    "# Aplicar la funciÃ³n a la columna 'amenities'\n",
    "df_featured['total_amenities'] = df_featured['amenities'].apply(count_amenities)\n",
    "\n",
    "# Verificar los primeros valores\n",
    "print(df_featured[['amenities', 'total_amenities']].head(10))\n",
    "\n",
    "# CorrelaciÃ³n con `log_price`\n",
    "correlation = df_featured[['total_amenities', 'log_price']].corr()\n",
    "print(\"\\nðŸ“ˆ CorrelaciÃ³n de total_amenities con log_price:\")\n",
    "print(correlation)\n",
    "\n",
    "# Guardar las columnas en un archivo CSV\n",
    "output_path = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/amenities_feature.csv\"\n",
    "df_featured[['amenities', 'total_amenities']].to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    latitude   longitude  avg_price_1km\n",
      "0  40.696524  -73.991617       4.990149\n",
      "1  40.766115  -73.989040       5.154065\n",
      "2  40.808110  -73.943756       4.700019\n",
      "3  37.772004 -122.431619       5.193322\n",
      "4  38.925627  -77.034596       4.944364\n",
      "5  37.753164 -122.429526       5.257195\n",
      "6  33.980454 -118.462821       5.227422\n",
      "7  34.046737 -118.260439       4.950736\n",
      "8  37.781128 -122.501095       5.006894\n",
      "9  33.992563 -117.895997       4.046230\n",
      "\n",
      "ðŸ“ˆ CorrelaciÃ³n de avg_price_1km con log_price:\n",
      "               avg_price_1km  log_price\n",
      "avg_price_1km       1.000000   0.531042\n",
      "log_price           0.531042   1.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calcular el precio promedio en un radio de 1KM (aproximado a 0.01 en coordenadas)\n",
    "def calculate_avg_price(lat, lon, df, radius=0.01):\n",
    "    nearby = df[(abs(df['latitude'] - lat) < radius) & (abs(df['longitude'] - lon) < radius)]\n",
    "    return nearby['log_price'].mean() if not nearby.empty else df['log_price'].mean()\n",
    "\n",
    "df_featured['avg_price_1km'] = df_featured.apply(lambda row: calculate_avg_price(row['latitude'], row['longitude'], df_featured), axis=1)\n",
    "\n",
    "# CorrelaciÃ³n con log_price\n",
    "correlation = df_featured[['avg_price_1km', 'log_price']].corr()\n",
    "\n",
    "# Mostrar los primeros valores de la nueva variable\n",
    "print(df_featured[['latitude', 'longitude', 'avg_price_1km']].head(10))\n",
    "\n",
    "# Mostrar la correlaciÃ³n\n",
    "print(\"\\nðŸ“ˆ CorrelaciÃ³n de avg_price_1km con log_price:\")\n",
    "print(correlation)\n",
    "\n",
    "# ðŸ“Œ Guardar las columnas en un archivo CSV\n",
    "output_path = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/ubi_feature.csv\"\n",
    "df_featured[['latitude', 'longitude', 'avg_price_1km']].to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ CorrelaciÃ³n de room_type con log_price:\n",
      "                           log_price\n",
      "log_price                   1.000000\n",
      "room_type_Entire home/apt   0.602500\n",
      "accommodates                0.567574\n",
      "bedrooms                    0.536171\n",
      "beds                        0.441953\n",
      "latitude                   -0.002193\n",
      "longitude                  -0.047529\n",
      "review_scores_rating       -0.070754\n",
      "room_type_Shared room      -0.222487\n",
      "room_type_Private room     -0.531648\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Cargar el dataset limpio\n",
    "file_path = '/Users/chants/Desktop/Pset2_cucalon/data/processed/Airbnb_Cleaned.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Copia del DataFrame para no modificar el original\n",
    "df_room_type = df.copy()\n",
    "\n",
    "# Verificar que `room_type` existe\n",
    "if 'room_type' not in df_room_type.columns:\n",
    "    print(\"âš ï¸ La columna 'room_type' no estÃ¡ en el DataFrame. Verifica el dataset.\")\n",
    "else:\n",
    "    # Aplicar One-Hot Encoding a `room_type`\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    encoded_data = encoder.fit_transform(df_room_type[['room_type']])\n",
    "\n",
    "    # Convertir el encoding en DataFrame\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['room_type']))\n",
    "\n",
    "    # Resetear Ã­ndices antes de concatenar\n",
    "    df_room_type.reset_index(drop=True, inplace=True)\n",
    "    encoded_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Concatenar con el dataset original y eliminar la columna original\n",
    "    df_room_type = pd.concat([df_room_type.drop(columns=['room_type']), encoded_df], axis=1)\n",
    "\n",
    "    # Verificar que todas las columnas sean numÃ©ricas\n",
    "    df_room_type = df_room_type.select_dtypes(include=['number'])\n",
    "\n",
    "    # Calcular la correlaciÃ³n con `log_price`\n",
    "    correlation_matrix = df_room_type.corr()\n",
    "    correlation_log_price = correlation_matrix[['log_price']].sort_values(by='log_price', ascending=False)\n",
    "\n",
    "    # Mostrar la correlaciÃ³n\n",
    "    print(\"\\nðŸ“ˆ CorrelaciÃ³n de room_type con log_price:\")\n",
    "    print(correlation_log_price)\n",
    "\n",
    "    # Guardar las columnas codificadas en un archivo CSV\n",
    "output_path = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/room_type_feature.csv\"\n",
    "df_room_type.to_csv(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ CorrelaciÃ³n de bed_type con log_price:\n",
      "                        log_price\n",
      "log_price                1.000000\n",
      "accommodates             0.567574\n",
      "bedrooms                 0.536171\n",
      "beds                     0.441953\n",
      "bed_type_Real Bed        0.099230\n",
      "latitude                -0.002193\n",
      "bed_type_Pull-out Sofa  -0.039481\n",
      "bed_type_Couch          -0.039985\n",
      "longitude               -0.047529\n",
      "bed_type_Airbed         -0.050427\n",
      "bed_type_Futon          -0.064557\n",
      "review_scores_rating    -0.070754\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# ðŸ“Œ Cargar el dataset limpio\n",
    "file_path = '/Users/chants/Desktop/Pset2_cucalon/data/processed/Airbnb_Cleaned.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ðŸ“Œ Copia del DataFrame para no modificar el original\n",
    "df_bed_type = df.copy()\n",
    "\n",
    "# ðŸ“Œ Aplicar One-Hot Encoding a `bed_type`\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # No eliminamos categorÃ­as para evitar pÃ©rdida de info\n",
    "encoded_data = encoder.fit_transform(df_bed_type[['bed_type']])\n",
    "\n",
    "# ðŸ“Œ Convertir el encoding en DataFrame con nombres de columnas correctos\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['bed_type']))\n",
    "\n",
    "# ðŸ“Œ Resetear Ã­ndices antes de concatenar\n",
    "df_bed_type.reset_index(drop=True, inplace=True)\n",
    "encoded_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# ðŸ“Œ Concatenar y eliminar la columna original\n",
    "df_bed_type = pd.concat([df_bed_type.drop(columns=['bed_type']), encoded_df], axis=1)\n",
    "\n",
    "# ðŸ“Œ Filtrar solo columnas numÃ©ricas para calcular la correlaciÃ³n\n",
    "df_numeric = df_bed_type.select_dtypes(include=['number'])\n",
    "\n",
    "# ðŸ“Œ Calcular la correlaciÃ³n con `log_price`\n",
    "correlation_matrix = df_numeric.corr()\n",
    "correlation_log_price = correlation_matrix[['log_price']].sort_values(by='log_price', ascending=False)\n",
    "\n",
    "# ðŸ“Œ Mostrar la correlaciÃ³n\n",
    "print(\"\\nðŸ“ˆ CorrelaciÃ³n de bed_type con log_price:\")\n",
    "print(correlation_log_price)\n",
    "\n",
    "# ðŸ“Œ Guardar las columnas codificadas en un archivo CSV\n",
    "output_path = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/bed_type_feature.csv\"\n",
    "df_bed_type.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ CorrelaciÃ³n de city con log_price:\n",
      "                      log_price\n",
      "log_price              1.000000\n",
      "accommodates           0.567574\n",
      "bedrooms               0.536171\n",
      "beds                   0.441953\n",
      "city_SF                0.166738\n",
      "city_DC                0.082282\n",
      "latitude              -0.002193\n",
      "longitude             -0.047529\n",
      "city_Chicago          -0.051902\n",
      "city_LA               -0.056585\n",
      "review_scores_rating  -0.070754\n",
      "city_NYC              -0.076958\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Cargar el dataset limpio\n",
    "file_path = '/Users/chants/Desktop/Pset2_cucalon/data/processed/Airbnb_Cleaned.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Copia del DataFrame para no modificar el original\n",
    "df_city = df.copy()\n",
    "\n",
    "# Aplicar One-Hot Encoding a `city`\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')  # Elimina la primera categorÃ­a para evitar colinealidad\n",
    "encoded_data = encoder.fit_transform(df_city[['city']])\n",
    "\n",
    "# Convertir el encoding en DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['city']))\n",
    "\n",
    "# Resetear los Ã­ndices antes de concatenar\n",
    "df_city.reset_index(drop=True, inplace=True)\n",
    "encoded_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Concatenar con el dataset original y eliminar la columna original\n",
    "df_city = pd.concat([df_city.drop(columns=['city']), encoded_df], axis=1)\n",
    "\n",
    "# Filtrar solo columnas numÃ©ricas para la correlaciÃ³n\n",
    "df_city_numeric = df_city.select_dtypes(include=['number'])\n",
    "\n",
    "# Calcular la correlaciÃ³n con `log_price`\n",
    "correlation_matrix = df_city_numeric.corr()\n",
    "correlation_log_price = correlation_matrix[['log_price']].sort_values(by='log_price', ascending=False)\n",
    "\n",
    "# Mostrar la correlaciÃ³n\n",
    "print(\"\\nðŸ“ˆ CorrelaciÃ³n de city con log_price:\")\n",
    "print(correlation_log_price)\n",
    "\n",
    "# Guardar las columnas codificadas en un archivo CSV\n",
    "output_path = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/city_feature.csv\"\n",
    "df_city.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ CorrelaciÃ³n de property_type con log_price:\n",
      "                                  log_price\n",
      "log_price                          1.000000\n",
      "accommodates                       0.567574\n",
      "bedrooms                           0.536171\n",
      "beds                               0.441953\n",
      "property_type_Condominium          0.065726\n",
      "property_type_Loft                 0.041135\n",
      "property_type_Timeshare            0.036887\n",
      "property_type_Villa                0.022659\n",
      "property_type_Other                0.020812\n",
      "property_type_Boat                 0.017296\n",
      "property_type_Boutique hotel       0.012821\n",
      "property_type_House                0.011228\n",
      "property_type_Townhouse            0.010967\n",
      "property_type_Vacation home        0.010851\n",
      "property_type_Castle               0.010826\n",
      "property_type_Serviced apartment   0.008575\n",
      "property_type_Earth House          0.005155\n",
      "property_type_Tipi                 0.004116\n",
      "property_type_Lighthouse           0.003786\n",
      "property_type_Train                0.003058\n",
      "property_type_Yurt                 0.002493\n",
      "property_type_In-law               0.001493\n",
      "property_type_Island               0.001170\n",
      "property_type_Cave                 0.000922\n",
      "property_type_Treehouse            0.000860\n",
      "property_type_Bungalow             0.000118\n",
      "property_type_Chalet              -0.000223\n",
      "property_type_Parking Space       -0.000906\n",
      "property_type_Casa particular     -0.002048\n",
      "latitude                          -0.002193\n",
      "property_type_Guest suite         -0.003669\n",
      "property_type_Cabin               -0.005893\n",
      "property_type_Camper/RV           -0.008933\n",
      "property_type_Hut                 -0.010996\n",
      "property_type_Guesthouse          -0.011108\n",
      "property_type_Tent                -0.012709\n",
      "property_type_Bed & Breakfast     -0.024951\n",
      "longitude                         -0.047529\n",
      "property_type_Hostel              -0.048635\n",
      "property_type_Dorm                -0.063270\n",
      "review_scores_rating              -0.070754\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Cargar el dataset limpio\n",
    "file_path = '/Users/chants/Desktop/Pset2_cucalon/data/processed/Airbnb_Cleaned.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Copia del DataFrame para no modificar el original\n",
    "df_property_type = df.copy()\n",
    "\n",
    "# Aplicar One-Hot Encoding a `property_type`\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')  \n",
    "encoded_data = encoder.fit_transform(df_property_type[['property_type']])\n",
    "\n",
    "# Convertir el encoding en DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['property_type']))\n",
    "\n",
    "# Resetear Ã­ndices antes de concatenar\n",
    "df_property_type.reset_index(drop=True, inplace=True)\n",
    "encoded_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Concatenar y eliminar la columna original\n",
    "df_property_type = pd.concat([df_property_type.drop(columns=['property_type']), encoded_df], axis=1)\n",
    "\n",
    "# Filtrar solo columnas numÃ©ricas para calcular la correlaciÃ³n\n",
    "df_numeric = df_property_type.select_dtypes(include=['number'])\n",
    "\n",
    "# Calcular la correlaciÃ³n con `log_price`\n",
    "correlation_matrix = df_numeric.corr()\n",
    "correlation_log_price = correlation_matrix[['log_price']].sort_values(by='log_price', ascending=False)\n",
    "\n",
    "# Mostrar la correlaciÃ³n\n",
    "print(\"\\nðŸ“ˆ CorrelaciÃ³n de property_type con log_price:\")\n",
    "print(correlation_log_price)\n",
    "\n",
    "# Guardar las columnas codificadas en un archivo CSV\n",
    "output_path = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/property_type_feature.csv\"\n",
    "df_property_type.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ CorrelaciÃ³n de cleaning_fee con log_price:\n",
      "                      log_price\n",
      "log_price              1.000000\n",
      "accommodates           0.567574\n",
      "bedrooms               0.536171\n",
      "beds                   0.441953\n",
      "cleaning_fee_True      0.111191\n",
      "latitude              -0.002193\n",
      "longitude             -0.047529\n",
      "review_scores_rating  -0.070754\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Cargar el dataset limpio\n",
    "file_path = '/Users/chants/Desktop/Pset2_cucalon/data/processed/Airbnb_Cleaned.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Copia del DataFrame para no modificar el original\n",
    "df_cleaning_fee = df.copy()\n",
    "\n",
    "# Aplicar One-Hot Encoding a `cleaning_fee`\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')  # drop='first' evita colinealidad\n",
    "encoded_data = encoder.fit_transform(df_cleaning_fee[['cleaning_fee']])\n",
    "\n",
    "# Convertir el encoding en DataFrame con nombres de columnas correctos\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['cleaning_fee']))\n",
    "\n",
    "# Resetear Ã­ndices antes de concatenar\n",
    "df_cleaning_fee.reset_index(drop=True, inplace=True)\n",
    "encoded_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Concatenar y eliminar la columna original\n",
    "df_cleaning_fee = pd.concat([df_cleaning_fee.drop(columns=['cleaning_fee']), encoded_df], axis=1)\n",
    "\n",
    "# Filtrar solo columnas numÃ©ricas para calcular la correlaciÃ³n\n",
    "df_numeric = df_cleaning_fee.select_dtypes(include=['number'])\n",
    "\n",
    "# Calcular la correlaciÃ³n con `log_price`\n",
    "correlation_matrix = df_numeric.corr()\n",
    "correlation_log_price = correlation_matrix[['log_price']].sort_values(by='log_price', ascending=False)\n",
    "\n",
    "# Mostrar la correlaciÃ³n\n",
    "print(\"\\nðŸ“ˆ CorrelaciÃ³n de cleaning_fee con log_price:\")\n",
    "print(correlation_log_price)\n",
    "\n",
    "# Guardar las columnas codificadas en un archivo CSV\n",
    "output_path = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/cleaning_fee_feature.csv\"\n",
    "df_cleaning_fee.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Archivo Airbnb_Featured.csv creado\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset original\n",
    "file_path = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/Airbnb_Cleaned.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Cargar los archivos de features generados previamente\n",
    "file_amenities = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/amenities_feature.csv\"\n",
    "file_ubi = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/ubi_feature.csv\"\n",
    "file_room = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/room_type_feature.csv\"\n",
    "file_bed = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/bed_type_feature.csv\"\n",
    "file_city = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/city_feature.csv\"\n",
    "file_property = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/property_type_feature.csv\"\n",
    "file_cleaning = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/cleaning_fee_feature.csv\"\n",
    "\n",
    "# Cargar los datasets de features\n",
    "df_amenities = pd.read_csv(file_amenities)\n",
    "df_ubi = pd.read_csv(file_ubi)\n",
    "df_room = pd.read_csv(file_room)\n",
    "df_bed = pd.read_csv(file_bed)\n",
    "df_city = pd.read_csv(file_city)\n",
    "df_property = pd.read_csv(file_property)\n",
    "df_cleaning = pd.read_csv(file_cleaning)\n",
    "\n",
    "# Unir las features al dataset original evitando duplicados\n",
    "df_featured = df.copy()\n",
    "\n",
    "df_featured = df_featured.merge(df_amenities, left_index=True, right_index=True, how=\"left\", suffixes=(\"\", \"_amen\"))\n",
    "df_featured = df_featured.merge(df_ubi, left_index=True, right_index=True, how=\"left\", suffixes=(\"\", \"_ubi\"))\n",
    "df_featured = df_featured.merge(df_room, left_index=True, right_index=True, how=\"left\", suffixes=(\"\", \"_room\"))\n",
    "df_featured = df_featured.merge(df_bed, left_index=True, right_index=True, how=\"left\", suffixes=(\"\", \"_bed\"))\n",
    "df_featured = df_featured.merge(df_city, left_index=True, right_index=True, how=\"left\", suffixes=(\"\", \"_city\"))\n",
    "df_featured = df_featured.merge(df_property, left_index=True, right_index=True, how=\"left\", suffixes=(\"\", \"_prop\"))\n",
    "df_featured = df_featured.merge(df_cleaning, left_index=True, right_index=True, how=\"left\", suffixes=(\"\", \"_clean\"))\n",
    "\n",
    "# Eliminar columnas duplicadas que se generaron en el merge\n",
    "df_featured = df_featured.loc[:, ~df_featured.columns.duplicated()]\n",
    "\n",
    "# Verificar si hay valores NaN y llenarlos con 0\n",
    "df_featured.fillna(0, inplace=True)\n",
    "\n",
    "# Seleccionar las mejores features\n",
    "selected_features = [\n",
    "    \"log_price\", \"avg_price_1km\", \"accommodates\", \"bedrooms\", \"beds\",\n",
    "    \"total_amenities\", \"city_SF\", \"city_DC\", \"property_type_Condominium\",\n",
    "    \"property_type_Loft\", \"cleaning_fee_True\"\n",
    "]\n",
    "\n",
    "# Filtrar solo las columnas necesarias\n",
    "df_selected = df_featured[selected_features]\n",
    "\n",
    "# Guardar el nuevo dataset con las features seleccionadas\n",
    "output_path = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/Airbnb_Featured.csv\"\n",
    "df_selected.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"âœ… Archivo Airbnb_Featured.csv creado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Archivo Airbnb_Featured_Scaled.csv creado con Ã©xito\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Cargar el dataset original\n",
    "file_path = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/Airbnb_Cleaned.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Cargar los archivos de features generados previamente\n",
    "file_amenities = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/amenities_feature.csv\"\n",
    "file_ubi = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/ubi_feature.csv\"\n",
    "file_room = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/room_type_feature.csv\"\n",
    "file_bed = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/bed_type_feature.csv\"\n",
    "file_city = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/city_feature.csv\"\n",
    "file_property = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/property_type_feature.csv\"\n",
    "file_cleaning = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/cleaning_fee_feature.csv\"\n",
    "\n",
    "# Cargar los datasets de features\n",
    "df_amenities = pd.read_csv(file_amenities)\n",
    "df_ubi = pd.read_csv(file_ubi)\n",
    "df_room = pd.read_csv(file_room)\n",
    "df_bed = pd.read_csv(file_bed)\n",
    "df_city = pd.read_csv(file_city)\n",
    "df_property = pd.read_csv(file_property)\n",
    "df_cleaning = pd.read_csv(file_cleaning)\n",
    "\n",
    "# Unir las features al dataset original evitando duplicados\n",
    "df_featured = df.copy()\n",
    "df_featured = df_featured.merge(df_amenities, left_index=True, right_index=True, how=\"left\", suffixes=(\"\", \"_amen\"))\n",
    "df_featured = df_featured.merge(df_ubi, left_index=True, right_index=True, how=\"left\", suffixes=(\"\", \"_ubi\"))\n",
    "df_featured = df_featured.merge(df_room, left_index=True, right_index=True, how=\"left\", suffixes=(\"\", \"_room\"))\n",
    "df_featured = df_featured.merge(df_bed, left_index=True, right_index=True, how=\"left\", suffixes=(\"\", \"_bed\"))\n",
    "df_featured = df_featured.merge(df_city, left_index=True, right_index=True, how=\"left\", suffixes=(\"\", \"_city\"))\n",
    "df_featured = df_featured.merge(df_property, left_index=True, right_index=True, how=\"left\", suffixes=(\"\", \"_prop\"))\n",
    "df_featured = df_featured.merge(df_cleaning, left_index=True, right_index=True, how=\"left\", suffixes=(\"\", \"_clean\"))\n",
    "\n",
    "# Eliminar columnas duplicadas que se generaron en el merge\n",
    "df_featured = df_featured.loc[:, ~df_featured.columns.duplicated()]\n",
    "\n",
    "# Verificar si hay valores NaN y llenarlos con 0\n",
    "df_featured.fillna(0, inplace=True)\n",
    "\n",
    "# Seleccionar las mejores features\n",
    "selected_features = [\n",
    "    \"log_price\", \"avg_price_1km\", \"accommodates\", \"bedrooms\", \"beds\",\n",
    "    \"total_amenities\", \"city_SF\", \"city_DC\", \"property_type_Condominium\",\n",
    "    \"property_type_Loft\", \"cleaning_fee_True\"\n",
    "]\n",
    "\n",
    "# Filtrar solo las columnas necesarias\n",
    "df_selected = df_featured[selected_features]\n",
    "\n",
    "# Separar `log_price` (objetivo) del resto de variables\n",
    "X = df_selected.drop(columns=[\"log_price\"])\n",
    "y = df_selected[\"log_price\"]\n",
    "\n",
    "# Aplicar StandardScaler solo a las variables numÃ©ricas\n",
    "num_features = [\"avg_price_1km\", \"accommodates\", \"bedrooms\", \"beds\", \"total_amenities\"]\n",
    "scaler = StandardScaler()\n",
    "X[num_features] = scaler.fit_transform(X[num_features])\n",
    "\n",
    "# Volver a unir `log_price` con las features escaladas\n",
    "df_final = X.copy()\n",
    "df_final[\"log_price\"] = y  # Mantener log_price sin escalar\n",
    "\n",
    "# Guardar el dataset final con las features escaladas\n",
    "output_path = \"/Users/chants/Desktop/Pset2_cucalon/data/processed/Airbnb_Featured_Scaled.csv\"\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"âœ… Archivo Airbnb_Featured_Scaled.csv creado con Ã©xito\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
